# -*- coding: utf-8 -*-
"""test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ApqgyYTouzwCzr3-Dkqo2kFnaPsbNlTr
"""

from dotenv import load_dotenv
import os
import chromadb

#.env file load
load_dotenv()

HOST = os.getenv('CHROMADB_HOST')
PORT = os.getenv('CHROMADB_PORT')
COLLECTION_NAME = os.getenv('CHROMADB_COLLECTION')
UPSTAGE_KEY = os.getenv('UPSTAGE_API_KEY')
UPSTAGE_KEY_TM = os.getenv('UPSTAGE_API_KEY_TM')

client = chromadb.HttpClient(host=HOST, port=PORT)
collection = client.get_or_create_collection(COLLECTION_NAME)

print(collection.get(include=['embeddings']))

import re

# 텍스트 전처리 함수
def clean_text(text):
    """텍스트 전처리 개선"""
    # 페이지 번호 제거
    text = re.sub(r'-\s*\d+\s*-', '', text)
    text = re.sub(r'\s*\d+\s*\n', '', text)

    # 불필요한 특수문자 제거
    text = re.sub(r'[•◆▶▷→▪]', '', text)

    # 삭제하고 싶은 특정 문구들
    remove_patterns = [
        r'도로교통법\s*\n',
        r'.*법제처.*\n',
        r'국가법령정보센터\s*\n',
        r'confusion matrix.*?(?=\n|$)',  # confusion matrix 관련 텍스트
        r'[rR][oO][cC].*?(?=\n|$)',     # ROC 관련 텍스트
    ]

    # 모든 패턴을 하나의 정규식으로 결합
    combined_pattern = '|'.join(remove_patterns)
    text = re.sub(combined_pattern, '', text)

    # 불필요한 숫자 패턴 제거
    text = re.sub(r'^\d+\.\s*', '', text, flags=re.MULTILINE)

    # 연속된 공백 정리
    text = re.sub(r'\s+', ' ', text)

    # 불필요한 줄바꿈 정리
    text = re.sub(r'\n+', '\n', text)

    # URL 패턴 정리
    text = re.sub(r'http\S+\s?', '', text)

    return text.strip()

from langchain_community.embeddings import HuggingFaceEmbeddings

# 임베딩 모델 생성
embeddings_model = HuggingFaceEmbeddings(
        model_name='sentence-transformers/all-MiniLM-L6-v2', # 한국어 자연어 추론에 최적화된 모델
        model_kwargs={'device':'cpu'}, # 모델이 CPU에서 실행되도록 설정 GPU를 사용할 수 있다면 cuda로 설정
        encode_kwargs={'normalize_embeddings':True}, # 임베딩을 정규화하여 모든 벡터가 같은 범위 값을 갖도록 한다.
    )

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

"""
7월 15일 도로교통법.pdf PyPDFLoader/RecursiveCharacterTextSplitter 사용하여 청킹 후
chromaDB 서버 rag_exam 컬렉션에 저장 완료
"""

# PDF 로드 및 청킹 후 임베딩하여 벡터 디비 저장
pdf_path = "docs/도로교통법.pdf"
loader = PyPDFLoader(pdf_path)
pages = loader.load()

# 메타데이터 설정
for page in pages:
    # 텍스트 전처리 적용
    page.page_content = clean_text(page.page_content)

# 텍스트 분할 설정
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,        # 청크 크기
    chunk_overlap=100,      # 청크 간 중복
    length_function=len,
    separators=[".", "\n\n"] # 분할 기준
)

# 전체 텍스트를 하나로 합쳐서 분할
full_text = '\n'.join([page.page_content for page in pages])
chunks = text_splitter.split_text(full_text)

print(f"\n총 {len(chunks)}개의 청크로 분할되었습니다.")

for i,chunk in enumerate(chunks):
    embedding = embeddings_model.embed_query(chunk)
    print(embedding)

    collection.add(
        embeddings = [embedding],
        documents = [chunk],
        ids = [f"chunk_{i}"]
    )

# 사용자 질문 반환 함수
def get_user_query():
    lines = []

    while True:
        line = input("질문을 입력하세요 (종료: 빈 줄에서 Enter)")
        # 빈 줄이고 이전에 입력된 내용이 있으면 종료
        if not line and lines:
            break
        # 입력된 줄이 있으면 추가
        if line:
            lines.append(line)

    return "\n".join(lines)

user_query = get_user_query()

user_query_embedding = embeddings_model.embed_query(user_query)
k = 2

results = collection.query(
    query_embeddings=[user_query_embedding],
    n_results=k,
    include=['documents']
)
print(user_query)

for i, chunk_id in enumerate(results['ids'][0]):
    print(f"{i}번째 청크 ID: {chunk_id}")
    print(f"해당 청크 내용: {results['documents'][0][i]}")  # 내용 앞부분만
    print('-------\n')

import httpx


contexts = []

for i, doc in enumerate(results['documents'][0]):
    contexts.append(f"{i + 1}번째 관련 문서 : \n{doc}\n")

context_text = "\n".join(contexts)

# 프롬프트 생성
prompt = f"""질문: {user_query}
참고할 도로교통법 내용 :
{context_text}

위 도로교통법 내용을 바탕으로 질문에 답변해주시기 바랍니다.
"""
api_key = UPSTAGE_KEY_TM
base_url = "https://api.upstage.ai/v1/chat/completions"
payload = {
    "model": "solar-pro",
    "messages": [
        {
            "role": "user",
            "content": prompt
        }
    ],
    "stream": False
}

headers = {
    "Authorization": f"Bearer {api_key}",
    "Content-Type": "application/json"
}

# timeout -> context가 길어 5초로는 부족, 60초로 늘려줌
with httpx.Client(timeout=60.0) as client:
    response = client.post(base_url, headers=headers, json=payload)

# 응답 결과 출력
print(response.status_code)
print(response.json())

data = response.json()

print(data['choices'][0]['message']['content'])